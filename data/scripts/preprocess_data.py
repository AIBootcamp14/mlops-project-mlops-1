# data/scripts/preprocess_data.py
import pandas as pd
import re
import os
import logging
from datetime import datetime
from pathlib import Path
from nltk.stem.porter import PorterStemmer
from sklearn.model_selection import train_test_split

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
try:
    import nltk
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

class SimpleDataPreprocessor:
    """ê°„ë‹¨í•œ ë°ì´í„° ì „ì²˜ë¦¬ ìë™í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.setup_logging()
        self.stemmer = PorterStemmer()
        self.setup_directories()
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'preprocessing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = ['data/raw', 'data/processed', 'logs']
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def load_data(self):
        """ë°ì´í„° ë¡œë”© - ë…¸íŠ¸ë¶ê³¼ ë™ì¼í•œ ë°©ì‹"""
        self.logger.info("ë°ì´í„° ë¡œë”© ì‹œì‘")
        
        data_path = 'data/spam.csv'
        
        # ì¸ì½”ë”© ìë™ ê°ì§€ ì‹œë„ (ë…¸íŠ¸ë¶ê³¼ ë™ì¼)
        try:
            df = pd.read_csv(data_path, encoding='utf-8')
            self.logger.info("UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        except UnicodeDecodeError:
            df = pd.read_csv(data_path, encoding='latin-1')
            self.logger.info("Latin-1 ì¸ì½”ë”©ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        
        self.logger.info(f"ë¡œë“œëœ ë°ì´í„° í˜•íƒœ: {df.shape}")
        return df
    
    def clean_columns(self, df):
        """ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€ - ë…¸íŠ¸ë¶ê³¼ ë™ì¼"""
        self.logger.info("ì»¬ëŸ¼ ì •ë¦¬ ì‹œì‘")
        
        # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° ë° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€
        if 'Unnamed: 0' in df.columns:
            df = df[['target', 'text']].copy()
        else:
            # ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
            possible_target_cols = ['target', 'label', 'class', 'spam']
            possible_text_cols = ['text', 'message', 'email', 'content']
            
            target_col = None
            text_col = None
            
            for col in df.columns:
                if col.lower() in possible_target_cols:
                    target_col = col
                elif col.lower() in possible_text_cols:
                    text_col = col
            
            if target_col and text_col:
                df = df[[target_col, text_col]].copy()
                df.columns = ['target', 'text']  # í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ìŒ ë‘ ì»¬ëŸ¼ ì‚¬ìš©
                df = df.iloc[:, :2].copy()
                df.columns = ['target', 'text']
        
        self.logger.info(f"ì •ë¦¬ëœ ë°ì´í„° í˜•íƒœ: {df.shape}")
        self.logger.info(f"ì»¬ëŸ¼: {list(df.columns)}")
        
        return df
    
    def process_target(self, df):
        """íƒ€ê²Ÿ ê°’ ë§¤í•‘ ë° ê²°ì¸¡ì¹˜/íƒ€ì… ì²˜ë¦¬ - ë…¸íŠ¸ë¶ê³¼ ë™ì¼"""
        self.logger.info("íƒ€ê²Ÿ ê°’ ì²˜ë¦¬ ì‹œì‘")
        
        # íƒ€ê²Ÿ ê°’ ë§¤í•‘ ('ham':0, 'spam':1)
        df['target'] = df['target'].map({'ham': 0, 'spam': 1}).fillna(df['target'])
        
        # ë§¤í•‘ ì‹¤íŒ¨ ë˜ëŠ” NaN ê°’ í–‰ ì œê±°
        before_count = len(df)
        df.dropna(subset=['target'], inplace=True)
        after_count = len(df)
        
        if before_count != after_count:
            self.logger.info(f"ê²°ì¸¡ì¹˜ ì œê±°: {before_count - after_count}ê°œ í–‰ ì œê±°")
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
        df['target'] = df['target'].astype(int)
        
        # íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
        target_counts = df['target'].value_counts()
        self.logger.info(f"íƒ€ê²Ÿ ë¶„í¬: {target_counts.to_dict()}")
        
        return df
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ - ë…¸íŠ¸ë¶ê³¼ ì™„ì „íˆ ë™ì¼"""
        if not isinstance(text, str):  # ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
            return ""
        text = text.lower()  # ì†Œë¬¸ì ë³€í™˜
        text = re.sub(r'[^a-z]', ' ', text)  # ì•ŒíŒŒë²³ ì™¸ ì œê±°
        text = text.split()  # ë‹¨ì–´ ë¶„ë¦¬
        text = [self.stemmer.stem(word) for word in text]  # ì–´ê°„ ì¶”ì¶œ
        return ' '.join(text)
    
    def apply_text_preprocessing(self, df):
        """ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©í•˜ì—¬ ìƒˆ ì»¬ëŸ¼ ìƒì„± - ë…¸íŠ¸ë¶ê³¼ ë™ì¼"""
        self.logger.info("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘")
        
        # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©í•˜ì—¬ ìƒˆ ì»¬ëŸ¼ ìƒì„±
        df['processed_text'] = df['text'].apply(self.preprocess_text)
        
        # ì „ì²˜ë¦¬ ê²°ê³¼ í™•ì¸
        self.logger.info("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ")
        self.logger.info("ì „ì²˜ë¦¬ ì˜ˆì‹œ:")
        for i in range(min(3, len(df))):
            self.logger.info(f"ì›ë³¸: {df.iloc[i]['text'][:50]}...")
            self.logger.info(f"ì „ì²˜ë¦¬: {df.iloc[i]['processed_text'][:50]}...")
            self.logger.info("---")
        
        return df
    
    def split_and_save_data(self, df):
        """ë°ì´í„° ë¶„í•  ë° ì €ì¥"""
        self.logger.info("ë°ì´í„° ë¶„í•  ë° ì €ì¥ ì‹œì‘")
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë³µì‚¬
        df_processed = df[['target', 'processed_text']].copy()
        
        # íƒ€ê²Ÿ ê²°ì¸¡ì¹˜ í–‰ ì œê±° (ê²¬ê³ ì„±ì„ ìœ„í•´ ë‹¤ì‹œ í¬í•¨)
        df_processed.dropna(subset=['target'], inplace=True)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (75:25) - ë…¸íŠ¸ë¶ê³¼ ë™ì¼
        X_train, X_test, y_train, y_test = train_test_split(
            df_processed['processed_text'], 
            df_processed['target'], 
            test_size=0.25, 
            random_state=42
        )
        
        # ë°ì´í„°ì…‹ í˜•íƒœ ì¶œë ¥
        self.logger.info(f"X_train í˜•íƒœ: {X_train.shape}")
        self.logger.info(f"X_test í˜•íƒœ: {X_test.shape}")
        self.logger.info(f"y_train í˜•íƒœ: {y_train.shape}")
        self.logger.info(f"y_test í˜•íƒœ: {y_test.shape}")
        
        # DataFrame í˜•íƒœë¡œ ì¬êµ¬ì„±
        train_df = pd.DataFrame({
            'text': X_train,
            'target': y_train
        })
        
        test_df = pd.DataFrame({
            'text': X_test,
            'target': y_test
        })
        
        # ì „ì²´ ì²˜ë¦¬ëœ ë°ì´í„°ë„ ì €ì¥
        full_processed_df = df_processed.copy()
        full_processed_df.columns = ['target', 'text']  # ì»¬ëŸ¼ëª… í†µì¼
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        processed_dir = Path('data/processed')
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ íŒŒì¼
        train_df.to_csv(processed_dir / f'train_data_{timestamp}.csv', index=False)
        test_df.to_csv(processed_dir / f'test_data_{timestamp}.csv', index=False)
        full_processed_df.to_csv(processed_dir / f'full_processed_data_{timestamp}.csv', index=False)
        
        # ìµœì‹  íŒŒì¼ (ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ë„ë¡)
        train_df.to_csv(processed_dir / 'train_data_latest.csv', index=False)
        test_df.to_csv(processed_dir / 'test_data_latest.csv', index=False)
        full_processed_df.to_csv(processed_dir / 'processed_data_latest.csv', index=False)
        
        self.logger.info("ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        self.logger.info(f"  - í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ ìƒ˜í”Œ")
        self.logger.info(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ìƒ˜í”Œ")
        self.logger.info(f"  - ì „ì²´ ë°ì´í„°: {len(full_processed_df)}ê°œ ìƒ˜í”Œ")
        
        return train_df, test_df, full_processed_df
    
    def generate_report(self, df, train_df, test_df):
        """ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'original_data_shape': df.shape,
            'processed_data_shape': (len(train_df) + len(test_df), 2),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_distribution': {
                'train': train_df['target'].value_counts().to_dict(),
                'test': test_df['target'].value_counts().to_dict()
            }
        }
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        import json
        with open('data/processed/preprocessing_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        self.logger.info("=== ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸ ===")
        self.logger.info(f"ì²˜ë¦¬ ì‹œê°„: {report['timestamp']}")
        self.logger.info(f"ì›ë³¸ ë°ì´í„°: {report['original_data_shape']}")
        self.logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„°: {report['processed_data_shape']}")
        self.logger.info(f"í›ˆë ¨ ìƒ˜í”Œ: {report['train_samples']}")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {report['test_samples']}")
        self.logger.info(f"í›ˆë ¨ íƒ€ê²Ÿ ë¶„í¬: {report['target_distribution']['train']}")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë¶„í¬: {report['target_distribution']['test']}")
    
    def run_preprocessing_pipeline(self):
        """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            self.logger.info("=== ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
            
            # 1. ë°ì´í„° ë¡œë”©
            df = self.load_data()
            
            # 2. ì»¬ëŸ¼ ì •ë¦¬
            df = self.clean_columns(df)
            
            # 3. íƒ€ê²Ÿ ê°’ ì²˜ë¦¬
            df = self.process_target(df)
            
            # 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            df = self.apply_text_preprocessing(df)
            
            # 5. ë°ì´í„° ë¶„í•  ë° ì €ì¥
            train_df, test_df, full_df = self.split_and_save_data(df)
            
            # 6. ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_report(df, train_df, test_df)
            
            self.logger.info("=== ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            
            return {
                'status': 'success',
                'train_samples': len(train_df),
                'test_samples': len(test_df),
                'total_samples': len(full_df)
            }
            
        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    preprocessor = SimpleDataPreprocessor()
    result = preprocessor.run_preprocessing_pipeline()
    
    print("\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {result['train_samples']}ê°œ ìƒ˜í”Œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {result['test_samples']}ê°œ ìƒ˜í”Œ")
    print(f"âœ… ì´ ë°ì´í„°: {result['total_samples']}ê°œ ìƒ˜í”Œ")
    print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print("  - data/processed/train_data_latest.csv")
    print("  - data/processed/test_data_latest.csv")
    print("  - data/processed/processed_data_latest.csv")
    print("  - data/processed/preprocessing_report.json")


if __name__ == "__main__":
    main()