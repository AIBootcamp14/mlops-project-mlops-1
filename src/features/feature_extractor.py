# src/features/feature_extractor.py
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import pickle
import joblib

class WorkingFeatureExtractor:
    def __init__(self):
        self.setup_logging()
        self.vectorizers = {}
    
    def setup_logging(self):
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def extract_basic_features(self, texts):
        self.logger.info("ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        features = pd.DataFrame()
        texts_clean = texts.fillna('')

        features['text_length'] = texts_clean.str.len()
        word_counts = texts_clean.str.split().str.len()
        features['word_count'] = word_counts.fillna(0)
        features['avg_word_length'] = features['text_length'] / features['word_count'].replace(0, 1)

        features['upper_count'] = texts_clean.str.count(r'[A-Z]')
        features['upper_ratio'] = features['upper_count'] / features['text_length'].replace(0, 1)

        features['digit_count'] = texts_clean.str.count(r'\d')
        features['digit_ratio'] = features['digit_count'] / features['text_length'].replace(0, 1)

        features['exclamation_count'] = texts_clean.apply(lambda x: str(x).count('!'))
        features['question_count'] = texts_clean.apply(lambda x: str(x).count('?'))
        features['at_count'] = texts_clean.apply(lambda x: str(x).count('@'))

        spam_words = ['free', 'win', 'call', 'click', 'urgent', 'now']
        for word in spam_words:
            features[f'{word}_count'] = texts_clean.str.lower().str.count(word)

        self.logger.info(f"ê¸°ë³¸ íŠ¹ì§• {len(features.columns)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
        return features

    def extract_tfidf_features(self, texts, fit=True):
        self.logger.info("TF-IDF íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        texts_clean = texts.fillna('')

        if fit:
            vectorizer = TfidfVectorizer(
                max_features=5508,  # ëª¨ë¸ í•™ìŠµ ì‹œ ì €ì¥ëœ feature ìˆ˜ë¡œ ê³ ì •
                ngram_range=(1, 2),
                stop_words='english',
                min_df=2,
                max_df=0.95
            )
            features = vectorizer.fit_transform(texts_clean)
            self.vectorizers['tfidf'] = vectorizer
            self.logger.info(f"TF-IDF ë²¡í„°ë¼ì´ì € í•™ìŠµ ì™„ë£Œ: {features.shape}")
        else:
            vectorizer = self.vectorizers['tfidf']
            features = vectorizer.transform(texts_clean)
            self.logger.info(f"TF-IDF íŠ¹ì§• ë³€í™˜ ì™„ë£Œ: {features.shape}")

        return features

    def apply_dimensionality_reduction(self, features, fit=True):
        self.logger.info("ì°¨ì› ì¶•ì†Œ ì ìš© ì¤‘...")

        if fit:
            svd = TruncatedSVD(n_components=100, random_state=42)
            reduced = svd.fit_transform(features)
            self.vectorizers['svd'] = svd
            self.logger.info(f"SVD ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {features.shape} -> {reduced.shape}")
        else:
            svd = self.vectorizers['svd']
            reduced = svd.transform(features)
            self.logger.info(f"SVD ë³€í™˜ ì™„ë£Œ: {features.shape} -> {reduced.shape}")

        return reduced

    def extract_all_features(self, df, fit=True):
        self.logger.info("=== ì „ì²´ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘ ===")

        basic_features = self.extract_basic_features(df['text'])
        tfidf_features = self.extract_tfidf_features(df['text'], fit=fit)
        reduced_tfidf = self.apply_dimensionality_reduction(tfidf_features, fit=fit)

        basic_array = np.nan_to_num(basic_features.values)

        combined_features = np.hstack([basic_array, reduced_tfidf])

        self.logger.info(f"íŠ¹ì§• ê²°í•© ì™„ë£Œ:")
        self.logger.info(f"  - ê¸°ë³¸ íŠ¹ì§•: {basic_array.shape}")
        self.logger.info(f"  - ì¶•ì†Œëœ TF-IDF: {reduced_tfidf.shape}")
        self.logger.info(f"  - ê²°í•©ëœ íŠ¹ì§•: {combined_features.shape}")

        return combined_features, basic_features

    def save_vectorizers(self, save_path='models/feature_extractor.joblib'):
        self.logger.info(f"ë²¡í„°ë¼ì´ì € ì €ì¥ ì¤‘: {save_path}")
        joblib.dump(self.vectorizers, save_path)
        self.logger.info("ë²¡í„°ë¼ì´ì € ì €ì¥ ì™„ë£Œ")

    def load_vectorizers(self, load_path='models/feature_extractor.joblib'):
        self.logger.info(f"ë²¡í„°ë¼ì´ì € ë¡œë”© ì¤‘: {load_path}")
        self.vectorizers = joblib.load(load_path)
        self.logger.info("ë²¡í„°ë¼ì´ì € ë¡œë”© ì™„ë£Œ")

def main():
    print("ğŸš€ ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œê¸° ì‹œì‘!")
    extractor = WorkingFeatureExtractor()

    train_path = 'data/processed/train_data_latest.csv'
    test_path = 'data/processed/test_data_latest.csv'

    if not Path(train_path).exists() or not Path(test_path).exists():
        print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {train_df.shape}")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")

    print("\nğŸ”§ í›ˆë ¨ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    X_train, train_basic = extractor.extract_all_features(train_df, fit=True)

    print("\nğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§• ë³€í™˜ ì¤‘...")
    X_test, test_basic = extractor.extract_all_features(test_df, fit=False)

    print("\nğŸ’¾ íŠ¹ì§• ë°ì´í„° ì €ì¥ ì¤‘...")
    features_dir = Path('data/processed/features')
    features_dir.mkdir(exist_ok=True)

    np.save(features_dir / 'X_train_features.npy', X_train)
    np.save(features_dir / 'X_test_features.npy', X_test)
    np.save(features_dir / 'y_train.npy', train_df['target'].values)
    np.save(features_dir / 'y_test.npy', test_df['target'].values)

    train_basic.to_csv(features_dir / 'train_basic_features.csv', index=False)
    test_basic.to_csv(features_dir / 'test_basic_features.csv', index=False)

    extractor.save_vectorizers()

    print("\n" + "="*60)
    print("ğŸ‰ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“ˆ í›ˆë ¨ íŠ¹ì§•: {X_train.shape}")
    print(f"ğŸ“ˆ í…ŒìŠ¤íŠ¸ íŠ¹ì§•: {X_test.shape}")
    print(f"ğŸ“ˆ ê¸°ë³¸ íŠ¹ì§•: {len(train_basic.columns)}ê°œ")
    print("\nğŸ”¤ ì¶”ì¶œëœ ê¸°ë³¸ íŠ¹ì§•:")
    for i, feature in enumerate(train_basic.columns, 1):
        print(f"  {i:2d}. {feature}")

if __name__ == "__main__":
    main()
