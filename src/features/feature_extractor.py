# src/features/feature_extractor_working.py
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import pickle
import json

class WorkingFeatureExtractor:
    def __init__(self):
        self.setup_logging()
        self.vectorizers = {}
        
    def setup_logging(self):
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def extract_basic_features(self, texts):
        """ê¸°ë³¸ íŠ¹ì§•ë§Œ ì¶”ì¶œ - ì•ˆì „í•œ ë²„ì „"""
        self.logger.info("ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        
        features = pd.DataFrame()
        texts_clean = texts.fillna('')
        
        # ê¸¸ì´ íŠ¹ì§•
        features['text_length'] = texts_clean.str.len()
        word_counts = texts_clean.str.split().str.len()
        features['word_count'] = word_counts.fillna(0)
        features['avg_word_length'] = features['text_length'] / features['word_count'].replace(0, 1)
        
        # ëŒ€ë¬¸ì íŠ¹ì§•  
        features['upper_count'] = texts_clean.str.count(r'[A-Z]')
        features['upper_ratio'] = features['upper_count'] / features['text_length'].replace(0, 1)
        
        # ìˆ«ì íŠ¹ì§•
        features['digit_count'] = texts_clean.str.count(r'\d')
        features['digit_ratio'] = features['digit_count'] / features['text_length'].replace(0, 1)
        
        # ê°„ë‹¨í•œ íŠ¹ìˆ˜ë¬¸ì (ì•ˆì „í•œ ë°©ì‹)
        features['exclamation_count'] = texts_clean.apply(lambda x: str(x).count('!'))
        features['question_count'] = texts_clean.apply(lambda x: str(x).count('?'))
        features['at_count'] = texts_clean.apply(lambda x: str(x).count('@'))
        
        # ìŠ¤íŒ¸ í‚¤ì›Œë“œ (ê°„ë‹¨í•œ ë²„ì „)
        spam_words = ['free', 'win', 'call', 'click', 'urgent', 'now']
        for word in spam_words:
            features[f'{word}_count'] = texts_clean.str.lower().str.count(word)
        
        self.logger.info(f"ê¸°ë³¸ íŠ¹ì§• {len(features.columns)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
        return features
    
    def extract_tfidf_features(self, texts, fit=True):
        """TF-IDF íŠ¹ì§• ì¶”ì¶œ"""
        self.logger.info("TF-IDF íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        
        if fit:
            self.vectorizers['tfidf'] = TfidfVectorizer(
                max_features=2000,
                ngram_range=(1, 2),
                stop_words='english',
                min_df=2,
                max_df=0.95
            )
            features = self.vectorizers['tfidf'].fit_transform(texts.fillna(''))
            self.logger.info(f"TF-IDF ë²¡í„°ë¼ì´ì € í•™ìŠµ ì™„ë£Œ: {features.shape}")
        else:
            features = self.vectorizers['tfidf'].transform(texts.fillna(''))
            self.logger.info(f"TF-IDF íŠ¹ì§• ë³€í™˜ ì™„ë£Œ: {features.shape}")
        
        return features
    
    def apply_dimensionality_reduction(self, features, fit=True):
        """ì°¨ì› ì¶•ì†Œ"""
        self.logger.info("ì°¨ì› ì¶•ì†Œ ì ìš© ì¤‘...")
        
        if fit:
            self.vectorizers['svd'] = TruncatedSVD(n_components=100, random_state=42)
            reduced = self.vectorizers['svd'].fit_transform(features)
            self.logger.info(f"SVD ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {features.shape} -> {reduced.shape}")
        else:
            reduced = self.vectorizers['svd'].transform(features)
            self.logger.info(f"SVD ë³€í™˜ ì™„ë£Œ: {features.shape} -> {reduced.shape}")
        
        return reduced
    
    def extract_all_features(self, df, fit=True):
        """ëª¨ë“  íŠ¹ì§• ì¶”ì¶œ ë° ê²°í•©"""
        self.logger.info("=== ì „ì²´ íŠ¹ì§• ì¶”ì¶œ ì‹œì‘ ===")
        
        # 1. ê¸°ë³¸ íŠ¹ì§•
        basic_features = self.extract_basic_features(df['text'])
        
        # 2. TF-IDF íŠ¹ì§•
        tfidf_features = self.extract_tfidf_features(df['text'], fit=fit)
        
        # 3. ì°¨ì› ì¶•ì†Œ
        reduced_tfidf = self.apply_dimensionality_reduction(tfidf_features, fit=fit)
        
        # 4. íŠ¹ì§• ê²°í•©
        basic_array = basic_features.values
        basic_array = np.nan_to_num(basic_array)  # NaN ì²˜ë¦¬
        
        combined_features = np.hstack([
            basic_array,
            reduced_tfidf
        ])
        
        self.logger.info(f"íŠ¹ì§• ê²°í•© ì™„ë£Œ:")
        self.logger.info(f"  - ê¸°ë³¸ íŠ¹ì§•: {basic_array.shape}")
        self.logger.info(f"  - ì¶•ì†Œëœ TF-IDF: {reduced_tfidf.shape}")
        self.logger.info(f"  - ê²°í•©ëœ íŠ¹ì§•: {combined_features.shape}")
        
        return combined_features, basic_features
    
    def save_vectorizers(self, save_path='data/processed/vectorizers.pkl'):
        """ë²¡í„°ë¼ì´ì € ì €ì¥"""
        self.logger.info(f"ë²¡í„°ë¼ì´ì € ì €ì¥ ì¤‘: {save_path}")
        
        save_data = {
            'vectorizers': self.vectorizers,
            'timestamp': datetime.now().isoformat()
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump(save_data, f)
        
        self.logger.info("ë²¡í„°ë¼ì´ì € ì €ì¥ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œê¸° ì‹œì‘!")
    
    extractor = WorkingFeatureExtractor()
    
    # ë°ì´í„° ë¡œë“œ
    train_path = 'data/processed/train_data_latest.csv'
    test_path = 'data/processed/test_data_latest.csv'
    
    if not Path(train_path).exists() or not Path(test_path).exists():
        print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    print(f"âœ… í›ˆë ¨ ë°ì´í„°: {train_df.shape}")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")
    
    # íŠ¹ì§• ì¶”ì¶œ
    print("\nğŸ”§ í›ˆë ¨ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    X_train, train_basic = extractor.extract_all_features(train_df, fit=True)
    
    print("\nğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§• ë³€í™˜ ì¤‘...")
    X_test, test_basic = extractor.extract_all_features(test_df, fit=False)
    
    # ì €ì¥
    print("\nğŸ’¾ íŠ¹ì§• ë°ì´í„° ì €ì¥ ì¤‘...")
    features_dir = Path('data/processed/features')
    features_dir.mkdir(exist_ok=True)
    
    # NumPy ë°°ì—´ ì €ì¥
    np.save(features_dir / 'X_train_features.npy', X_train)
    np.save(features_dir / 'X_test_features.npy', X_test)
    np.save(features_dir / 'y_train.npy', train_df['target'].values)
    np.save(features_dir / 'y_test.npy', test_df['target'].values)
    
    # ê¸°ë³¸ íŠ¹ì§• ì €ì¥ (ë¶„ì„ìš©)
    train_basic.to_csv(features_dir / 'train_basic_features.csv', index=False)
    test_basic.to_csv(features_dir / 'test_basic_features.csv', index=False)
    
    # ë²¡í„°ë¼ì´ì € ì €ì¥
    extractor.save_vectorizers()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ‰ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“ˆ í›ˆë ¨ íŠ¹ì§•: {X_train.shape}")
    print(f"ğŸ“ˆ í…ŒìŠ¤íŠ¸ íŠ¹ì§•: {X_test.shape}")
    print(f"ğŸ“ˆ ê¸°ë³¸ íŠ¹ì§•: {len(train_basic.columns)}ê°œ")
    
    print("\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print("  - data/processed/features/X_train_features.npy")
    print("  - data/processed/features/X_test_features.npy")
    print("  - data/processed/features/y_train.npy")
    print("  - data/processed/features/y_test.npy")
    print("  - data/processed/vectorizers.pkl")
    
    print("\nğŸ”¤ ì¶”ì¶œëœ ê¸°ë³¸ íŠ¹ì§•:")
    for i, feature in enumerate(train_basic.columns, 1):
        print(f"  {i:2d}. {feature}")
    
    
if __name__ == "__main__":
    main()